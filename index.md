- [Формулы](#formula)
- [Теория](#theory)

<a name="formula"></a> 
### Формулы

![image](https://github.com/user-attachments/assets/453bb478-a92a-425f-8881-cc81827a4233)

![image](https://github.com/user-attachments/assets/70357dff-c5d4-4650-ba6e-67fff8074aef)

![image](https://github.com/user-attachments/assets/db758695-ef2d-4d8c-80a0-9f304b1defe8)

![image](https://github.com/user-attachments/assets/9bc4fb63-ce62-483e-b063-184374486ca0)

![image](https://github.com/user-attachments/assets/51201e94-a015-4d99-bb8c-e2c1b79f2596)

![image](https://github.com/user-attachments/assets/35fa191d-d300-43cf-b430-f1bac7d94968)

![image](https://github.com/user-attachments/assets/fefb15e0-6c2b-4e08-b884-acb9ad13fded)

![image](https://github.com/user-attachments/assets/305086ce-9406-4f7e-ae84-cdb3d34125ca)

![image](https://github.com/user-attachments/assets/72fcdb0e-4c5b-4419-abd7-169a0b1d42de)

![image](https://github.com/user-attachments/assets/7afd0b5a-3260-4177-b7ad-ad15218ed244)

![image](https://github.com/user-attachments/assets/2b979a89-88f9-4675-a24c-23843b8f1556)

<a name="theory"></a> 
### Теория
## Решение задачи классификации. Часть 1

### Метод k-ближайших соседей (KNN)
- **Идея**: Классификация нового объекта по классам ближайших \( k \) соседей.
- **Как работает**:
  - Выбирается количество ближайших соседей \( k \).
  - Для нового объекта находится расстояние до всех объектов выборки.
  - Присваивается класс, наиболее часто встречающийся среди соседей.
- **Гиперпараметр**: число \( k \).

---

### Деревья решений
- **Идея**: Построение дерева, где каждый узел — проверка условия, а листья — классы.
- **Как работает**:
  - Выбирается признак, который лучше всего разделяет данные.
  - Дерево строится рекурсивно, пока не достигнута заданная глубина или минимальное число объектов в узле.
- **Применение**: Удобны для интерпретации, но могут переобучаться.

---

### Случайный лес (Random Forest)
- **Идея**: Использует множество деревьев решений, каждое обучается на случайной подвыборке.
- **Особенности**:
  - Усредняет результаты всех деревьев (в классификации — голосование).
  - Устойчив к переобучению.
- **Гиперпараметры**: число деревьев, глубина дерева, случайные признаки.

---

### Метрики оценки качества
1. **Accuracy**: Доля правильно классифицированных объектов.
2. **Precision**: Доля объектов, предсказанных как положительные, которые действительно положительные.
3. **Recall**: Доля всех положительных объектов, которые были правильно найдены.
4. **F-мера**: Среднее гармоническое Precision и Recall.

---

## Решение задачи классификации. Часть 2

### Метод опорных векторов (SVM)
- **Идея**: Ищет гиперплоскость, которая максимально разделяет классы.
- **Особенности**:
  - Используется для линейных и нелинейных разделений.
  - Для нелинейных задач применяются ядра (например, RBF).
- **Гиперпараметры**: регуляризация, тип ядра.

---

### Логистическая регрессия
- **Идея**: Предсказывает вероятность принадлежности к классу с использованием логистической функции.
- **Особенности**:
  - Линейный метод.
  - Хорошо работает на простых задачах.

---

### Наивный Байесовский классификатор
- **Идея**: Основан на теореме Байеса. Предполагает независимость признаков.
- **Применение**: Простой и быстрый алгоритм для текстовой классификации.

---

### Метрики оценки качества
1. **Кросс-энтропия (Logloss)**: Используется для оценки качества вероятностных предсказаний.
2. **ROC-AUC**: Измеряет качество ранжирования объектов.

---

## Задача кластеризации

### Метод k-means
- **Идея**: Делит данные на \( k \) кластеров, минимизируя расстояния до центроидов.
- **Как работает**:
  - Инициализируются центроиды.
  - Объекты распределяются к ближайшим центроидам.
  - Центроиды пересчитываются, процесс повторяется.
- **Метод локтя**: Используется для выбора оптимального числа кластеров.

---

### Метод DBSCAN
- **Идея**: Выделяет плотные области и игнорирует выбросы.
- **Особенности**:
  - Хорошо работает с данными произвольной формы.
  - Требует задания радиуса \( \varepsilon \) и минимального числа точек для кластера.

---

### Иерархическая кластеризация
1. **Агломеративная**: Объединение кластеров снизу вверх.
2. **Дивизивная**: Разделение кластеров сверху вниз.
- **Применение**: Построение иерархической структуры данных.

---

### Метрики оценки качества
1. **Метрика силуэт**: Оценивает, насколько хорошо объекты принадлежат своему кластеру.
2. **Метод локтя**: Помогает определить оптимальное число кластеров, наблюдая за снижением внутрикластерной дисперсии.

---
