- [Формулы](#formula)
- [Теория](#theory)

<a name="formula"></a> 
### Формулы
![image](https://github.com/user-attachments/assets/d6b3b396-6a9e-49d4-a414-04e867c9fa3f)

![image](https://github.com/user-attachments/assets/122afef1-a7b6-4dd6-a7f2-b2c5ba5cd20d)

![image](https://github.com/user-attachments/assets/9f6d9991-bf0b-455c-9e1e-5b3cfc61c84d)

![image](https://github.com/user-attachments/assets/7e662101-bc67-42dd-911d-0dab948af51b)

![image](https://github.com/user-attachments/assets/f6d4f597-6fb3-40c8-bf1c-ee7180abbbd5)

![image](https://github.com/user-attachments/assets/9cfb74a6-0a6c-4633-a0e1-8e2e6f2caffb)

![image](https://github.com/user-attachments/assets/156dcefc-1a46-4ceb-bb6c-75b3abf170e5)

![image](https://github.com/user-attachments/assets/22e5c7e2-6054-4979-a505-a6ef9cecd3ef)

![image](https://github.com/user-attachments/assets/01a9eada-6b65-480f-8b7b-568bbf868b5b)

![image](https://github.com/user-attachments/assets/fed792fd-ef9d-4a8f-9b79-609f256fe47e)

![image](https://github.com/user-attachments/assets/38bf04e2-ca72-4448-8739-503e9790bfd5)

![image](https://github.com/user-attachments/assets/9d0ecece-56d1-4304-a6f4-e88080037e6f)

![image](https://github.com/user-attachments/assets/f4890640-5619-4878-bb6b-be81b9d101c6)



<a name="theory"></a> 
### Теория
## Решение задачи классификации. Часть 1

### Метод k-ближайших соседей (KNN)
- **Идея**: Классификация нового объекта по классам ближайших \( k \) соседей.
- **Как работает**:
  - Выбирается количество ближайших соседей \( k \).
  - Для нового объекта находится расстояние до всех объектов выборки.
  - Присваивается класс, наиболее часто встречающийся среди соседей.
- **Гиперпараметр**: число \( k \).

---

### Деревья решений
- **Идея**: Построение дерева, где каждый узел — проверка условия, а листья — классы.
- **Как работает**:
  - Выбирается признак, который лучше всего разделяет данные.
  - Дерево строится рекурсивно, пока не достигнута заданная глубина или минимальное число объектов в узле.
- **Применение**: Удобны для интерпретации, но могут переобучаться.

---

### Случайный лес (Random Forest)
- **Идея**: Использует множество деревьев решений, каждое обучается на случайной подвыборке.
- **Особенности**:
  - Усредняет результаты всех деревьев (в классификации — голосование).
  - Устойчив к переобучению.
- **Гиперпараметры**: число деревьев, глубина дерева, случайные признаки.

---

### Метрики оценки качества
1. **Accuracy**: Доля правильно классифицированных объектов.
2. **Precision**: Доля объектов, предсказанных как положительные, которые действительно положительные.
3. **Recall**: Доля всех положительных объектов, которые были правильно найдены.
4. **F-мера**: Среднее гармоническое Precision и Recall.

---

## Решение задачи классификации. Часть 2

### Метод опорных векторов (SVM)
- **Идея**: Ищет гиперплоскость, которая максимально разделяет классы.
- **Особенности**:
  - Используется для линейных и нелинейных разделений.
  - Для нелинейных задач применяются ядра (например, RBF).
- **Гиперпараметры**: регуляризация, тип ядра.

---

### Логистическая регрессия
- **Идея**: Предсказывает вероятность принадлежности к классу с использованием логистической функции.
- **Особенности**:
  - Линейный метод.
  - Хорошо работает на простых задачах.

---

### Наивный Байесовский классификатор
- **Идея**: Основан на теореме Байеса. Предполагает независимость признаков.
- **Применение**: Простой и быстрый алгоритм для текстовой классификации.

---

### Метрики оценки качества
1. **Кросс-энтропия (Logloss)**: Используется для оценки качества вероятностных предсказаний.
2. **ROC-AUC**: Измеряет качество ранжирования объектов.

---

## Задача кластеризации

### Метод k-means
- **Идея**: Делит данные на \( k \) кластеров, минимизируя расстояния до центроидов.
- **Как работает**:
  - Инициализируются центроиды.
  - Объекты распределяются к ближайшим центроидам.
  - Центроиды пересчитываются, процесс повторяется.
- **Метод локтя**: Используется для выбора оптимального числа кластеров.

---

### Метод DBSCAN
- **Идея**: Выделяет плотные области и игнорирует выбросы.
- **Особенности**:
  - Хорошо работает с данными произвольной формы.
  - Требует задания радиуса \( \varepsilon \) и минимального числа точек для кластера.

---

### Иерархическая кластеризация
1. **Агломеративная**: Объединение кластеров снизу вверх.
2. **Дивизивная**: Разделение кластеров сверху вниз.
- **Применение**: Построение иерархической структуры данных.

---

### Метрики оценки качества
1. **Метрика силуэт**: Оценивает, насколько хорошо объекты принадлежат своему кластеру.
2. **Метод локтя**: Помогает определить оптимальное число кластеров, наблюдая за снижением внутрикластерной дисперсии.

---
